# -*- coding: utf-8 -*-
"""loan eligibility prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12h40_bBFIVnsZcrPqtvh1eOc_4EH7RLu

# Project Title : Loan Eligibility Prediction using Machine Learning

## Project Overview
This project aims to predict the eligibility of loan applicants based on their personal, financial, and employment details. The model helps banks and financial institutions automate and streamline the loan approval process by making data-driven decisions.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## Loading the Dataset"""

loan = pd.read_csv('/content/loan_approval_dataset.csv')
loan.sample(5)

loan.shape

loan.info()

loan.describe()

loan.rename(columns=lambda x: x.lower().strip(), inplace=True)
print(loan.columns)

# Checking the datatypes of the columns
loan.dtypes

"""## Hanidling with the missing values"""

# In the loan dataset their is no Null values
# If in case is their any null values is their for Categorial values change with Mode/ffill/bfill and for Numerical value change with mean/median as per the data requirements
loan.isnull().sum()

"""# Exploratory Data Analysis

## Univariate Analysis
"""

loan['loan_status'].value_counts().plot.bar()

loan['no_of_dependents'].value_counts(normalize=True).plot.pie(title= 'no_of_dependents',autopct='%1.1f%%',figsize=(8, 5))

"""## Bivariate Analysis"""

education=pd.crosstab(loan['education'],loan['loan_status'])
education

education.plot(kind="bar", stacked=True, figsize=(6,6))
plt.show()

self_employed=pd.crosstab(loan['self_employed'],loan['loan_status'])
self_employed

self_employed.plot(kind="bar", stacked=True, figsize=(6,6))
plt.show()

Dependents=pd.crosstab(loan['no_of_dependents'],loan['loan_status'])
Dependents

Dependents.plot(kind="bar", stacked=True, figsize=(6,6))
plt.show()

"""## Distribution of the variables"""

loan['cibil_score'].hist(bins=50)

"""## Checking the outliers for the numerical columns"""

numerical_cols = loan.select_dtypes(include=['int64', 'float64']).columns
plt.figure(figsize=(15, 5))
loan[numerical_cols].boxplot(rot=45, grid=True)

# Checking the outliers for the numerical columns
numerical_cols = loan.select_dtypes(include=['int64', 'float64']).columns
fig, axes = plt.subplots(2, 3, figsize=(16, 8), constrained_layout=True)  # 2 rows, 3 columns
for ax, column in zip(axes.flat, numerical_cols):
    sns.histplot(data=loan, x=column, ax=ax, kde=True, color='gray', bins=15)  # Add KDE if desired
    ax.set_title(f'Histogram for {column}', fontsize=10)
    ax.set_xlabel('')

"""## Label Encoding Processing"""

from sklearn.preprocessing import LabelEncoder, StandardScaler

"""### Convert Categorical Variables to Numerical

"""

lc = LabelEncoder()
for col in ['education','self_employed','loan_status']:
    loan[col] = lc.fit_transform(loan[col])

"""### Standarizing the Numerical columns"""

scaler = StandardScaler()
for col in ['no_of_dependents','income_annum','loan_amount','loan_term','cibil_score','residential_assets_value','commercial_assets_value','luxury_assets_value','bank_asset_value']:
   loan[[col]] = scaler.fit_transform(loan[[col]])

# Stastical Summary
loan.describe()

"""### Train-Test Spliting the Data"""

x = loan.iloc[:,1:12]
x.shape

y = loan[['loan_status']]
y.shape

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state = 42)

"""## Model Selection & Implementation"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

"""## Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(x_train, y_train)

y_pred = lr.predict(x_test)
Accuraacy_score = accuracy_score(y_test, y_pred)
Train = lr.score(x_train,y_train)*100
cm = confusion_matrix(y_test, y_pred)
print('confusion_matrix:\n',cm)
report = classification_report(y_test, y_pred)
print('classification_report:\n',report)
print('Training_accuracy: ', Train)
print('Testing_accuracy: ',Accuraacy_score)

"""## Decision tree Model"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(x_train, y_train)

y_pred = dt.predict(x_test)
Accuraacy_score = accuracy_score(y_test, y_pred)
Train = dt.score(x_train,y_train)*100
cm = confusion_matrix(y_test, y_pred)
print('confusion_matrix:\n',cm)
report = classification_report(y_test, y_pred)
print('classification_report:\n',report)
print('Training_accuracy: ', Train)
print('Testing_accuracy: ',Accuraacy_score)

"""## KNN Model"""

from sklearn.neighbors import KNeighborsClassifier
knc = KNeighborsClassifier()
knc.fit(x_train, y_train)

y_pred = knc.predict(x_test)
Accuraacy_score = accuracy_score(y_test, y_pred)
Train = knc.score(x_train,y_train)*100
cm = confusion_matrix(y_test, y_pred)
print('confusion_matrix:\n',cm)
report = classification_report(y_test, y_pred)
print('classification_report:\n',report)
print('Training_accuracy: ', Train)
print('Testing_accuracy: ',Accuraacy_score)

"""## Naive Bayes Model"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(x_train, y_train)

y_pred = gnb.predict(x_test)
Accuraacy_score = accuracy_score(y_test, y_pred)
Train = gnb.score(x_train,y_train)*100
cm = confusion_matrix(y_test, y_pred)
print('confusion_matrix:\n',cm)
report = classification_report(y_test, y_pred)
print('classification_report:\n',report)
print('Training_accuracy: ', Train)
print('Testing_accuracy: ',Accuraacy_score)

"""## Support vector machine Model"""

from sklearn.svm import SVC
svc = SVC()
svc.fit(x_train, y_train)

y_pred = svc.predict(x_test)
Accuraacy_score = accuracy_score(y_test, y_pred)
Train = svc.score(x_train,y_train)*100
cm = confusion_matrix(y_test, y_pred)
print('confusion_matrix:\n',cm)
report = classification_report(y_test, y_pred)
print('classification_report:\n',report)
print('Training_accuracy: ', Train)
print('Testing_accuracy: ',Accuraacy_score)

models = [ 'Logistic Regression','Decision Tree','K-Nearest Neighbors','Naive Bayes','SVM']
train_accuracy = [92, 100, 95, 93, 95]
test_accuracy = [90, 98, 89, 93, 93]
x = np.arange(len(models))
width = 0.35
fig, ax = plt.subplots(figsize=(15,8))

# Plot bars
bars1 = ax.bar(x - width/2, train_accuracy, width, label='Training Accuracy', color='tab:blue')
bars2 = ax.bar(x + width/2, test_accuracy, width, label='Testing Accuracy', color='tab:orange')
ax.set_xlabel('Models')
ax.set_ylabel('Accuracy')
ax.set_title('Model Accuracy Comparison')
ax.set_xticks(x)
ax.set_xticklabels(models, rotation=30, ha="right")
ax.legend()
plt.show()







